{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Как разделить вывод зарплат на максимальный и минимальный, не нашел.\n",
    "Данные в датафрейм не вывел, запутался\n",
    "Суперджоб тоже не осилил\n",
    "С любопытством ожидаю разбор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T21:58:19.349530Z",
     "start_time": "2020-02-01T21:58:19.339437Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# hh ## next = '/search/vacancy?L_is_autosearch=false&area=1&clusters=true&enable_snippets=true&text=data&page='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Парсим hh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T01:30:42.603897Z",
     "start_time": "2020-02-01T01:30:42.585982Z"
    }
   },
   "outputs": [],
   "source": [
    "def hh_parse():# Этот вариант выводит список на печать. Следующий пишет в pandas\n",
    "    main_link = 'https://hh.ru'\n",
    "    param_link = '/search/vacancy?area=1&search_period=3&text=python&page='\n",
    "    page = '0'\n",
    "    headers = {'accept':'*/*',\n",
    "        'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36 OPR/66.0.3515.27'\n",
    "    }\n",
    "    link = main_link+param_link+page\n",
    "    \n",
    "    session = requests.Session()\n",
    "#    vac_count = 0\n",
    "    for i in range (0, 5):\n",
    "        page = str(i)\n",
    "        link = main_link+param_link+page\n",
    "        request = session.get(link, headers=headers)\n",
    "        if request.status_code == 200:\n",
    "            soup = bs(request.content, 'html.parser')\n",
    "            divs = soup.find_all('div', attrs={'data-qa':'vacancy-serp__vacancy'})\n",
    "            for div in divs:\n",
    "                title = div.find('a', attrs={'data-qa':'vacancy-serp__vacancy-title'}).text\n",
    "                href = div.find('a', attrs={'data-qa':'vacancy-serp__vacancy-title'})['href']\n",
    "                try:\n",
    "                    comp = div.find('div', attrs={'data-qa':'vacancy-serp__vacancy-compensation'}).text\n",
    "                except:\n",
    "                    comp = 'З/п не указана'\n",
    "\n",
    "                print('hh.ru', title, comp)\n",
    "                print(href, '\\n')\n",
    "#                vac_count += 1\n",
    "            time.sleep(random.randint(1,5))\n",
    "        \n",
    "        else:\n",
    "            print('ERROR')\n",
    "            \n",
    " #   print(vac_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T22:25:13.825733Z",
     "start_time": "2020-02-01T22:25:13.812101Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hh_parse_pd():# Этот вариант пишет в пандас\n",
    "    df = pd.DataFrame\n",
    "    main_link = 'https://hh.ru'\n",
    "    param_link = '/search/vacancy?area=1&search_period=3&text=python&page='\n",
    "    page = '0'\n",
    "    headers = {'accept':'*/*',\n",
    "        'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36 OPR/66.0.3515.27'\n",
    "    }\n",
    "    link = main_link+param_link+page\n",
    "    df = pd.DataFrame(columns=('Source', 'Title', 'Salary', 'Href')) \n",
    "    \n",
    "    session = requests.Session()\n",
    "#    vac_count = 0\n",
    "    for i in range (0, 5):\n",
    "        page = str(i)\n",
    "        link = main_link+param_link+page\n",
    "        request = session.get(link, headers=headers)\n",
    "        if request.status_code == 200:\n",
    "            soup = bs(request.content, 'html.parser')\n",
    "            divs = soup.find_all('div', attrs={'data-qa':'vacancy-serp__vacancy'})\n",
    "            for div in divs:\n",
    "                title = div.find('a', attrs={'data-qa':'vacancy-serp__vacancy-title'}).text\n",
    "                href = div.find('a', attrs={'data-qa':'vacancy-serp__vacancy-title'})['href']\n",
    "                try:\n",
    "                    comp = div.find('div', attrs={'data-qa':'vacancy-serp__vacancy-compensation'}).text\n",
    "                except:\n",
    "                    comp = 'З/п не указана'\n",
    "                spam = pd.Series({'source' : 'hh.ru', 'title' : title, 'comp' : comp, 'href' : href})\n",
    "                #df = df.append(spam, ignore_index=True)\n",
    "\n",
    "\n",
    "#                 print('hh.ru', title, comp)\n",
    "#                 print(href, '\\n')\n",
    "#                vac_count += 1\n",
    "            break\n",
    "            time.sleep(random.randint(1,5))\n",
    "        \n",
    "        else:\n",
    "            print('ERROR')\n",
    "    \n",
    "    return df.tail()\n",
    " #   print(vac_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T22:25:15.087366Z",
     "start_time": "2020-02-01T22:25:14.540609Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can only append a Series if ignore_index=True or if the Series has a name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-68be6144502f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhh_parse_pd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-e6e674591d65>\u001b[0m in \u001b[0;36mhh_parse_pd\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mspam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'source'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'hh.ru'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comp'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'href'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m#df = df.append(spam, ignore_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6656\u001b[0m                 \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6657\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6658\u001b[0;31m                 raise TypeError('Can only append a Series if ignore_index=True'\n\u001b[0m\u001b[1;32m   6659\u001b[0m                                 ' or if the Series has a name')\n\u001b[1;32m   6660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can only append a Series if ignore_index=True or if the Series has a name"
     ]
    }
   ],
   "source": [
    "print(hh_parse_pd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# **Парсим superjob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T01:21:12.184186Z",
     "start_time": "2020-02-01T01:21:10.927443Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Junior <span class=\"_1rS-s\">Data</span> Scientist</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\"><span class=\"_1rS-s\">Data</span> Scientist (с экспертизой в технологиях графа знаний)</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Аналитик (<span class=\"_1rS-s\">Data</span> Scientist)</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\"><span class=\"_1rS-s\">Data</span> Scientist</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Middle / Senior <span class=\"_1rS-s\">Data</span> Scientist</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Аналитик данных (<span class=\"_1rS-s\">Data</span> Analyst 1С)</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Аналитик <span class=\"_1rS-s\">Data</span> Scientist</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Ведущий <span class=\"_1rS-s\">data</span> scientist</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Аналитик данных (<span class=\"_1rS-s\">Data</span> Scientist)</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\"><span class=\"_1rS-s\">Data</span> Scientist / Machine Learning Engineer</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\"><span class=\"_1rS-s\">Data</span> Science-Researcher (CV)</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Специалист по информационной безопасности</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Лидер команды разработки (Scala) / Руководитель команды разработки</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">JAVA-разработчик</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Старший специалист по продажам B2B</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Старший специалист по продажам B2B (средний и малый бизнес)</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Ведущий специалист по продажам B2B (средний и малый бизнес)</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Веб-программист .Net (C#)</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Менеджер по продажам крупным корпоративным клиентам</div>, <div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\">Ведущий специалист по развитию крупных корпоративных клиентов</div>]\n"
     ]
    }
   ],
   "source": [
    "def sj_parse():\n",
    "    main_link = 'https://www.superjob.ru'\n",
    "    page = '1'\n",
    "    position = 'data'\n",
    "    #link = main_link+param_link+page\n",
    "    #print(parsed_html)\n",
    "    headers = {'accept':'*/*',\n",
    "        'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36 OPR/66.0.3515.27'\n",
    "    }\n",
    "    req = requests.get(main_link+f'/vacancy/search/?keywords={position}&page={page}&geo%5Bc%5D%5B0%5D=1',headers=headers)\n",
    "    soup = bs(req.content, 'html.parser')\n",
    "    divs = soup.find_all('div', attrs={'class':'_3mfro CuJz5 PlM3e _2JVkc _3LJqf'})\n",
    "    \n",
    "#     for div in divs:\n",
    "#         title = div.find('div', attrs={'class':'_3mfro CuJz5 PlM3e _2JVkc _3LJqf'}).text\n",
    "    \n",
    "    print(divs)\n",
    "\n",
    "sj_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#<div class=\"_3mfro CuJz5 PlM3e _2JVkc _3LJqf\"><span class=\"_1rS-s\">Data</span> Scientist</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T00:58:56.352776Z",
     "start_time": "2020-02-01T00:58:56.349430Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def sj_parse(link, headers):\n",
    "#     main_link = 'https://www.superjob.ru'\n",
    "#     param_link = '/vacancy/search/?keywords=\"data\"&geo%5Bc%5D%5B0%5D=1&page=1'\n",
    "#     page = '1'\n",
    "#     headers = {'accept':'*/*',\n",
    "#         'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36 OPR/66.0.3515.27'\n",
    "#     }\n",
    "    \n",
    "#     session = requests.Session()\n",
    "# #    vac_count = 0\n",
    "#     for i in range (1, 6):\n",
    "#         page = str(i)\n",
    "#         link = main_link+param_link+page\n",
    "#         request = session.get(link, headers=headers)\n",
    "#         if request.status_code == 200:\n",
    "            \n",
    "#             soup = bs(request.content, 'html.parser')\n",
    "#             #divs = soup.find_all('div', attrs={'data-qa':'vacancy-serp__vacancy'})\n",
    "#             #divs = soup.find_all('div', attrs={'class':'_3zucV _2GPIV f-test-vacancy-item i6-sc _3VcZr'})\n",
    "#             print(soup)\n",
    "\n",
    "#             #for div in divs:\n",
    "#                 #title = div.find('a', attrs={'data-qa':'vacancy-serp__vacancy-title'}).text\n",
    "#                 #href = div.find('a', attrs={'data-qa':'vacancy-serp__vacancy-title'})['href']\n",
    "#                 #try:\n",
    "#                 #    comp = div.find('div', attrs={'data-qa':'vacancy-serp__vacancy-compensation'}).text\n",
    "#                 #except:\n",
    "#                 #    comp = 'З/п не указана'\n",
    "\n",
    "#                 #print('hh.ru', title, comp)\n",
    "#                 #print(href, '\\n')\n",
    "# #                vac_count += 1\n",
    "#             #time.sleep(random.randint(1,5))\n",
    "#         break   \n",
    "#         #else:\n",
    "#          #   print('ERROR')\n",
    "            \n",
    "# print(sj_parse(link, headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
